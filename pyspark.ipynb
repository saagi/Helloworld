{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIGPFYCg1DGfSt/pL2+03T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saagi/Helloworld/blob/master/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq >> /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "amtusRPLtatC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/lib/jvm"
      ],
      "metadata": {
        "id": "fAtp4kqvtavx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77cf0537-54e0-44d0-ceae-ce84d3b909ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default-java\t\t   java-11-openjdk-amd64     java-8-openjdk-amd64\n",
            "java-1.11.0-openjdk-amd64  java-1.8.0-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdGdzN19sdpa",
        "outputId": "ea1147f8-0f60-443f-da7c-f146189871e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pyarrow"
      ],
      "metadata": {
        "id": "dDXFyCDQDakw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"]=\"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "ujQICvGO0Fzu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "KLuleX50DmKG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.executor.memory\",\"8g\")\n",
        "spark.conf.set(\"spark.driver.memory\",\"2g\")\n",
        "spark.conf.set(\"spark.memory.fraction\",\"0.9\")"
      ],
      "metadata": {
        "id": "FdXjwdauDmHD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1v0tvhXh61I0I48IdLE_gG175pdgRfUhV"
      ],
      "metadata": {
        "id": "IQe6YKAA0Fw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c31071-78b1-4a98-bcc6-5b73676d9f4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1v0tvhXh61I0I48IdLE_gG175pdgRfUhV\n",
            "To: /content/Technical_Round.zip\n",
            "100% 18.8M/18.8M [00:00<00:00, 36.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/Technical_Round.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJiT6scHERVc",
        "outputId": "a2ff3f25-b9e0-46d6-d9fb-09a58e68b536"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Technical_Round.zip\n",
            "  inflating: Technical_Round_v2/material_list.txt  \n",
            "  inflating: Technical_Round_v2/Data.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.options(header='true', inferSchema='true').csv(\"Technical_Round_v2/Data.csv\")\n",
        "df = df.limit(20000)"
      ],
      "metadata": {
        "id": "gTHwbwgxFBbv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv -P sample_data\n",
        "# pyspark practise\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LD71Pwd-S2gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.options(header=True,delimeter=';').csv(\"sample_data/winequality-red.csv\")"
      ],
      "metadata": {
        "id": "cvYAOXv6kkRz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "df.show(5,False)"
      ],
      "metadata": {
        "id": "_z-tf35tkkO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,lit,when"
      ],
      "metadata": {
        "id": "xKjQippSkkL7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.where(col(\"cancel_status\")!=1).show(5,False)\n",
        "df.select(col(\"name_1\").alias(\"Name\"),col(\"uom\"),col(\"postal_code\")).show(5,False)\n",
        "#df.selectExpr(\"name_1 as Name\",\"uom\",\"postal_code as PIN_code\").show(5,False)\n",
        "#df.select(df.columns).show(4,False)\n",
        "#len(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk2Q4AM3kkJW",
        "outputId": "8cb6fff6-ddb6-4733-a374-70dd45c7178d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+---+-----------+\n",
            "|Name                  |uom|postal_code|\n",
            "+----------------------+---+-----------+\n",
            "|o vidhya sagar        |KG |501401     |\n",
            "|Anushka fresh world   |KG |560067     |\n",
            "|Naveen provision store|KG |560035     |\n",
            "|Greens vegetables     |KG |562125     |\n",
            "|Anushka fresh world   |KG |560067     |\n",
            "+----------------------+---+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df.select(\"name_1\").distinct()\n",
        "df3=df.join(df2,df.name_1==df.name_1,\"inner\")\n",
        "#df3.count()\n",
        "#df.select(\"name_1\").where(\"name_1 is null\").count()\n",
        "#df.dropDuplicates([\"name_1\",\"cancel_status\"]).count()\n"
      ],
      "metadata": {
        "id": "vX6WCxOGkkHR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.where(\"delivery_date is not null\").orderBy(col(\"delivery_date\").desc()).show(4,False)\n",
        "#df.where(\"delivery_date is not null\").orderBy(col(\"delivery_date\").desc()).show(4,False)\n",
        "df.orderBy(col(\"delivery_date\").desc_nulls_first()).show(4,False)"
      ],
      "metadata": {
        "id": "MZbqGHaC5FIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"temp\")\n",
        "spark.sql(\"\"\"\n",
        "select * from temp where delivery_date is not null and cancel_status=1 order by delivery_date desc\n",
        "\"\"\").show(5,False)"
      ],
      "metadata": {
        "id": "wGJO9ZUD5E0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min,max,sum,count\n",
        "#df.groupBy(\"cancel_status\").count().show(5,False) -> will give overall count\n",
        "#df.groupBy(\"cancel_status\").max().show(3,False)  -> will give max of each column\n",
        "df.groupBy(\"cancel_status\").agg(sum(\"order_quantity\").alias(\"sum_quantity\"),count(\"order_quantity\").alias(\"count_orders\")).show(10,False)\n",
        "\n"
      ],
      "metadata": {
        "id": "3njm2MgMCe_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Joins\n",
        "df1=df.select(\"name_1\",\"cancel_status\").distinct()\n",
        "df2=df.select(\"name_1\").distinct()\n",
        "df3=df1.join(df2,df1.name_1==df2.name_1,\"outer\")\n",
        "\n",
        "#df3.show(5,False)\n",
        "df1.join(df2,df1.name_1==df2.name_1,\"leftsemi\").show(5,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SX5DRuk5ExU",
        "outputId": "f4668b87-e985-47c8-893d-ba28eea0aff7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+-------------+\n",
            "|name_1                |cancel_status|\n",
            "+----------------------+-------------+\n",
            "|o vidhya sagar        |1            |\n",
            "|Anushka fresh world   |1            |\n",
            "|Naveen provision store|1            |\n",
            "|Greens vegetables     |1            |\n",
            "|r s provision store   |1            |\n",
            "+----------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## PartitionBy and repartitions..\n",
        "# partitionBy(\"<Column>\") -> creates each sub directory wrt each partition <Column>\n",
        "# repartition(k) -> in each sub directory there are k files each representing each partition\n",
        "\n",
        "df.rdd.getNumPartitions()\n",
        "df=df.repartition(10)\n",
        "df.rdd.getNumPartitions()\n",
        "df.write.partitionBy(\"cancel_status\").options(header=True).mode(\"overwrite\").csv(\"/tmp/dfpart\")"
      ],
      "metadata": {
        "id": "WwVz1cHskkFH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /tmp/dfpart/'cancel_status=1'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7kTbXSptjmM",
        "outputId": "b03ab55e-6289-4b3d-fd01-a37aa3518d78"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n",
            "part-00001-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n",
            "part-00002-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n",
            "part-00003-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n",
            "part-00004-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n",
            "part-00005-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n",
            "part-00006-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n",
            "part-00007-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n",
            "part-00008-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n",
            "part-00009-9fb8e61a-df2c-4bd5-ab63-622193a5dbb1.c000.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSxgzhbPtjjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbA5f3kZtjg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZ0KrtZMkkCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.select('delivery_date', 'cancel_status', 'material_description', 'material_no', 'plant', 'distribution_channel', 'uom','order_quantity')"
      ],
      "metadata": {
        "id": "iaC2ZkKXS2c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df,test_df=df.randomSplit([0.8,0.2],seed=12)\n",
        "train_df.count(),test_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZbhPwaGjL2r",
        "outputId": "98e808ac-7feb-4c6e-d30c-1af555ee0816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16045, 3955)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.where(\"delivery_date like '2021-%' and uom is not null\").show(5,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59nHebQvjLz4",
        "outputId": "4a459fde-a412-4e31-8599-d9a627e2c7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------+---------------------+------------------+-----+--------------------+---+--------------+\n",
            "|delivery_date|cancel_status|material_description |material_no       |plant|distribution_channel|uom|order_quantity|\n",
            "+-------------+-------------+---------------------+------------------+-----+--------------------+---+--------------+\n",
            "|2021-11-19   |1            |APPLE GALA           |WC0000000000000161|1009 |52                  |KG |2.0           |\n",
            "|2021-11-19   |1            |APPLE GALA 4PC - PACK|WC0000000000000013|1009 |52                  |PAC|2.0           |\n",
            "|2021-11-19   |1            |APPLE GALA 4PC - PACK|WC0000000000000013|1009 |52                  |PAC|4.0           |\n",
            "|2021-11-19   |1            |APPLE GALA 4PC - PACK|WC0000000000000013|1009 |52                  |PAC|4.0           |\n",
            "|2021-11-19   |1            |APPLE GALA 4PC - PACK|WC0000000000000013|1009 |52                  |PAC|5.0           |\n",
            "+-------------+-------------+---------------------+------------------+-----+--------------------+---+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7yuLOuwFS2ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJwPZL-IS2YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_8eFvBMFBYr",
        "outputId": "39d66fa4-0673-4c85-be8f-b099b66b8192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "709334"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.select('delivery_date', 'cancel_status', 'material_description', 'material_no', 'plant', 'distribution_channel', 'uom','order_quantity')"
      ],
      "metadata": {
        "id": "u49uBOsIuY0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf2eOTrvuZAd",
        "outputId": "ddc76d4f-710a-4fdc-f950-942c32e1eb5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.printSchema of DataFrame[delivery_date: string, cancel_status: string, material_description: string, material_no: string, plant: int, distribution_channel: int, uom: string, order_quantity: double]>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.where(\"material_description like '%BHENDI%' and plant='1000' and delivery_Date>='2021-08-16 and delivery_Date<=2021-11-25'\").orderBy(\"delivery_date\").show(5,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niT2pE5quZCX",
        "outputId": "ee9bbabb-c9f0-461a-ce95-6c1a68f6f981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------+----------------------+------------------+-----+--------------------+---+--------------+\n",
            "|delivery_date|cancel_status|material_description  |material_no       |plant|distribution_channel|uom|order_quantity|\n",
            "+-------------+-------------+----------------------+------------------+-----+--------------------+---+--------------+\n",
            "|2021-08-17   |1            |BHENDI (LADIES FINGER)|WC0000000000230022|1000 |50                  |KG |3.0           |\n",
            "|2021-08-17   |1            |BHENDI (LADIES FINGER)|WC0000000000230022|1000 |50                  |KG |6.0           |\n",
            "|2021-08-17   |1            |BHENDI (LADIES FINGER)|WC0000000000230022|1000 |51                  |KG |5.0           |\n",
            "|2021-08-17   |1            |BHENDI (LADIES FINGER)|WC0000000000230022|1000 |50                  |KG |3.0           |\n",
            "|2021-08-17   |1            |BHENDI (LADIES FINGER)|WC0000000000230022|1000 |50                  |KG |3.0           |\n",
            "+-------------+-------------+----------------------+------------------+-----+--------------------+---+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.where(\"material_description like '%BHENDI%' and plant='1000' and delivery_Date>='2021-08-16 and delivery_Date<=2021-11-25'\").groupBy(\"delivery_Date\").count().orderBy(\"delivery_Date\").show(15,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc_ood4qwEgd",
        "outputId": "ed74a727-15d4-450b-a3c8-b9e7da29a00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|delivery_Date|count|\n",
            "+-------------+-----+\n",
            "|2021-08-17   |28   |\n",
            "|2021-08-18   |23   |\n",
            "|2021-08-19   |18   |\n",
            "|2021-08-20   |26   |\n",
            "|2021-08-21   |21   |\n",
            "|2021-08-22   |14   |\n",
            "|2021-08-23   |7    |\n",
            "|2021-08-24   |27   |\n",
            "|2021-08-25   |10   |\n",
            "|2021-08-26   |21   |\n",
            "|2021-08-27   |21   |\n",
            "|2021-08-28   |15   |\n",
            "|2021-08-29   |30   |\n",
            "|2021-08-30   |11   |\n",
            "|2021-08-31   |34   |\n",
            "+-------------+-----+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.where(\"delivery_Date like '2021%'\").selectExpr(\"min(delivery_Date) as min_date\").show(2,False)"
      ],
      "metadata": {
        "id": "NE9hKBrzuZEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b92ff0-6757-4edc-99bd-39b2fcbf0200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|min_date  |\n",
            "+----------+\n",
            "|2021-07-01|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.selectExpr(\"max(delivery_Date) as max_date\").show(2,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xroJzhSi2Pl6",
        "outputId": "7d273240-6522-4530-b1fc-42e860fd080d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|max_date  |\n",
            "+----------+\n",
            "|2021-11-25|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFVaCFGW8aFk",
        "outputId": "f7c035b9-58a8-4d18-fb8d-0e29875a568a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.repartition(10)\n",
        "df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiktaHvi9AMW",
        "outputId": "51056cba-ff5a-48c4-ab28-8f546dc77b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.coalesce(5)\n",
        "df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g57CSUwH9UvO",
        "outputId": "a8d05c36-298b-4ddb-bb72-4fce17268379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROvo-IoL9AJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc,sum,col\n",
        "df2=df.groupBy(\"material_no\").count().orderBy(desc(\"count\"))"
      ],
      "metadata": {
        "id": "qe4zUWgCuZIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3=df.select(\"material_description\",\"material_no\").distinct()"
      ],
      "metadata": {
        "id": "jSiVVvjH3iUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jxQmkd85FBWc",
        "outputId": "873f5340-43c3-4dca-abe1-22611cdd8f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o297.join.\n: org.apache.spark.sql.AnalysisException: Reference 'material_no' is ambiguous, could be: material_no, material_no.;\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$41.apply(Analyzer.scala:900)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$41.apply(Analyzer.scala:902)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:899)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$35.apply(Analyzer.scala:968)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$35.apply(Analyzer.scala:968)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:968)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:911)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:911)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:756)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.join(Dataset.scala:1001)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-cfc08b02261d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"material_no\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be basestring\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'material_no' is ambiguous, could be: material_no, material_no.;\""
          ]
        }
      ]
    }
  ]
}